# Machine Learning Basics
This folder contains implementation of some of the most basic and/or fundemantal algorithms in ML. The implementation, in most cases, takes a brute force approach i.e. not optimized enough versus a standard package.

## 1.Outlier Detection
This specific folder contains implementation of some outlier detection modules, ranging from naive statistical methods to more advanced methods including a NN architecture. The goal here is to build a repository of approaches as individual scripts and make them availabe. Again, there are optimized packages that might do the same but often one will encounter datasets that are hard to process using the standard packages or needs special methods. Building these algorithms from scratch allows it to be easily extensible for those one-off cases.

### a. Stats_od
This script detects outlier using three simple statistical approaches. The first approach assumes the distribution of the data to be normal and uses mean and standard deviation to calculate z-scores. These z-scores are then used to find outliers using a threshold of 3 standard deviations from the mean. The second approach does something similar but using inter-quartile range instead of mean using 1.5 times IQR as the measure to calculate the upper and lower bounds to look for outliers in the data. The last approach usese a modified z-score which choose the main measure of central tendency as median instead of mean. Modified z-scores are calculated using Median Absolute Deviation (MAD). The third approach is generally more robust than the other two because median is not as sensitive to outliers as mean and standard deviation. In general, though, the statistical approaches make assumptions that may not always hold so they should be used more a a baseline model for low dimensional data. As the number of dimensions of the data get bigger, the data points tend to look more similar. Another issue with statistical tests is that the reference view is global and hence needs a good amount of data to be useful which also means that the model flexibility is low.

### b. Knn_od
This script uses K-nearest neighbor implmentation, defaulting to using Euclidean distance, to find outliers by looking for points that are further from most number of data points(using counts). While clustering is a good baseline option(certainly better than statistical for higher dimension data), it has its own drawbacks. First, there isn't an easy or automatic way to define k or how many of the top farthest points we should consider as outliers. Clustering algorithms are also generally optimized to find clusters rather than outliers. A set of data points that are outliers may together be classified as a cluster, which is especially problematic if there are many outliers. Clustering algorithms are also complex in terms of execution and can significantly slow down findding outliers for big data sized sets. Keeping this in mind, it may be a good starting point to use methods like these for simpler dataset or as a baseline but may not be ideal at scale. 