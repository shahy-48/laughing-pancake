# Machine Learning Basics
This folder contains implementation of some of the most basic and/or fundemantal algorithms in ML. The implementation, in most cases, takes a brute force approach i.e. not optimized enough versus a standard package.

## 1.Outlier Detection
This specific folder contains implementation of some outlier detection modules, ranging from naive statistical methods to more advanced methods including a NN architecture. The goal here is to build a repository of approaches as individual scripts and make them availabe. Again, there are optimized packages that might do the same but often one will encounter datasets that are hard to process using the standard packages or needs special methods. Building these algorithms from scratch allows it to be easily extensible for those one-off cases.

### a. Stats_od
This script detects outlier using three simple statistical approaches. The first approach assumes the distribution of the data to be normal and uses mean and standard deviation to calculate z-scores. These z-scores are then used to find outliers using a threshold of 3 standard deviations from the mean. The second approach does something similar but using inter-quartile range instead of mean using 1.5 times IQR as the measure to calculate the upper and lower bounds to look for outliers in the data. The last approach usese a modified z-score which choose the main measure of central tendency as median instead of mean. Modified z-scores are calculated using Median Absolute Deviation (MAD). The third approach is generally more robust than the other two because median is not as sensitive to outliers as mean and standard deviation. In general, though, the statistical approaches make assumptions that may not always hold so they should be used more a a baseline model for low dimensional data. As the number of dimensions of the data get bigger, the data points tend to look more similar. Another issue with statistical tests is that the reference view is global and hence needs a good amount of data to be useful which also means that the model flexibility is low.

### b. Knn_od
This script uses K-nearest neighbor implmentation, defaulting to using Euclidean distance, to find outliers by looking for points that are further from most number of data points(using counts). This method to find outliers works well when outliers are sparse. For a highly imbalanced dataset with many abnormal points vs normal in the training data, there is a risk that the abnormal datapoints might be classified as normal. Also, this is a supervised learning model and therefore needs labeled training data to learn from which is not great if the problem needs unsupervised learning methods.

### c. Gmm_od
Implementation of Gaussian Mixture Model using PyTorch. The implementation focuses on using GMM, which is essentially a clustering algorithm, to find outliers. The basic idea around GMM is that the data points come from various Gaussian distributions but we do not know how many and the source of each data point. So instead of a hard clustering algorithm like K-Means that has rigid clusters in circles or spheres, GMM uses soft clustering method using probabilities and covariance matrix. The use of covariance matrix helps it to define cluster geometry beyond a simple circle or sphere for higher dimensional data. In practice, we start with a set of random Gaussian parameters which is used to estimate the probability of each point belonging to that distribution. After assignment, we use these groups to recalculate and adjust the parameters and then start all over again. We use this process iteratively until convergence i.e. high likelihood that our Gaussian distributions fit the actual distributions well. One problem with using GMM is that we don't know how many Gaussians i.e. k distributions to use. Theoretically, we can use the BIC and AIC curves to determine a good choice but often in practice using a test and validation set to try out numerous value and assessing them using Likelihood works out to be a better alternative.


While clustering is a good baseline option(certainly better than basic statistical approaches for higher dimension data), it has its own drawbacks. First, there isn't an easy or automatic way to define k. Clustering algorithms are also generally optimized to find clusters rather than outliers. A set of data points that are outliers may together be classified as a cluster, which is especially problematic if there are many outliers. Clustering algorithms are also complex in terms of execution and can significantly slow down finding outliers for big data sized sets. Keeping this in mind, it may be a good starting point to use methods like these for simpler dataset or as a baseline but may not be ideal at scale. 